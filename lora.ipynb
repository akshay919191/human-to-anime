{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "40554a67",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\sharm\\miniconda3\\envs\\gpt\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torchvision.transforms as transforms\n",
    "import torch.nn as nn\n",
    "from diffusers import StableDiffusionImg2ImgPipeline\n",
    "import math\n",
    "from PIL import Image\n",
    "import os\n",
    "from torch.utils.data import Dataset , DataLoader\n",
    "import torch.nn.functional as F\n",
    "from torch.cuda.amp import autocast\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "17459272",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cannot initialize model with low cpu memory usage because `accelerate` was not found in the environment. Defaulting to `low_cpu_mem_usage=False`. It is strongly recommended to install `accelerate` for faster and less memory-intense model loading. You can do so with: \n",
      "```\n",
      "pip install accelerate\n",
      "```\n",
      ".\n",
      "Keyword arguments {'dtype': torch.float16} are not expected by StableDiffusionImg2ImgPipeline and will be ignored.\n",
      "Loading pipeline components...: 100%|██████████| 7/7 [00:00<00:00,  9.75it/s]\n"
     ]
    }
   ],
   "source": [
    "model_id = \"runwayml/stable-diffusion-v1-5\"\n",
    "pipe = StableDiffusionImg2ImgPipeline.from_pretrained(model_id , dtype = torch.float16).to(\"cuda\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "603099b8",
   "metadata": {},
   "source": [
    "we used this model as its quite stable and familiar with anime ,\n",
    "\n",
    "\n",
    "we will be modifying key and the value , not query and projection so basically attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "042d57b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LoRA(nn.Module):\n",
    "    def __init__(self , original , rank , alpha):\n",
    "        super().__init__()\n",
    "\n",
    "        self.original = original\n",
    "        self.in_feat = original.in_features\n",
    "        self.out_feat = original.out_features\n",
    "        self.scaling  = alpha / rank\n",
    "\n",
    "        self.lora_1 = nn.Parameter(torch.randn(self.in_feat , rank))\n",
    "        self.lora_2 = nn.Parameter(torch.zeros(rank , self.out_feat))\n",
    "\n",
    "    def forward(self , x):\n",
    "        return self.original(x) + torch.matmul(self.lora_2 , torch.matmul(self.lora_1 , x)) * self.scaling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53bd103f",
   "metadata": {},
   "source": [
    "we made lora 2 weights zero so it made the new condition same as model like N = Wx + bias , so we don't want to change in start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a77095d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe.unet.requires_grad_(False)\n",
    "pipe.unet.to('cuda' , dtype = torch.float16)\n",
    "target_layers = [\"to_k\", \"to_v\"]\n",
    "for name , layer in pipe.unet.named_modules():\n",
    "    if any(target in name for target in target_layers) and isinstance(layer , nn.Linear):\n",
    "        parent_name = \".\".join(name.split(\".\")[:-1])\n",
    "        layer_name = name.split(\".\")[-1]\n",
    "        parent = pipe.unet.get_submodule(parent_name)\n",
    "\n",
    "        new = LoRA(layer , rank = 8 , alpha = 8)\n",
    "\n",
    "        new.to(\"cuda\" , dtype = torch.float16)\n",
    "        setattr(parent , layer_name , new)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4096dbf",
   "metadata": {},
   "source": [
    "we changed the value of to_v and to_k to lora instead of linear"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6b3029b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Datasett(Dataset):\n",
    "    def __init__(self , tokenizer , img_dir , cap_info , size = 512):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.img_dir = img_dir\n",
    "        self.cap_dir = cap_info\n",
    "        self.size = size\n",
    "        self.images = [f for f in os.listdir(img_dir) if f.endswith((\"png\" , \"jpg\"))]\n",
    "        self.transform = transforms.Compose([\n",
    "            transforms.Resize((size, size)),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize([0.5] , [0.5])\n",
    "        ])\n",
    "    \n",
    "    def __len__(self): return len(self.images)\n",
    "    def __getitem__(self, index):\n",
    "        img = self.images[index]\n",
    "        img_path = os.path.join(self.img_dir , img)  \n",
    "        image = Image.open(img_path).convert('RGB')\n",
    "        image = self.transform(image)\n",
    "\n",
    "        name = [str(\"0\" * (4 - len(str(x))) + str(x)) for x in range(1 , 1892)]\n",
    "\n",
    "        file_name = name[index] + \".txt\"\n",
    "        file_path = os.path.join(self.cap_dir , file_name)\n",
    "\n",
    "        with open(file_path , \"r\")  as f:\n",
    "            caption = f.read().strip()\n",
    "        \n",
    "        token = self.tokenizer(caption , padding = \"max_length\" , truncation = True , return_tensors = \"pt\").input_ids[0]\n",
    "\n",
    "        return image , token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b6518cb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_da = Datasett(tokenizer = pipe.tokenizer , img_dir = r\"C:\\Users\\sharm\\Downloads\\dell\\finetune\\dataset_512\\anime_images\" , cap_info = r\"C:\\Users\\sharm\\Downloads\\dell\\finetune\\dataset_512\\info\")\n",
    "\n",
    "train_loader = DataLoader(train_da , batch_size = 4 , shuffle = True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "35500411",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe.unet.to(dtype = torch.float32)\n",
    "pipe.vae.to(dtype = torch.float32)\n",
    "params = [p for p in pipe.unet.parameters() if p.requires_grad]\n",
    "optimizer = torch.optim.Adam(params , lr = 1e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f78aa420",
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(2):\n",
    "    running_loss = 0.0\n",
    "    pipe.unet.train()\n",
    "    for image , cap in train_loader:\n",
    "        image , cap = image.to(\"cuda\" , dtype = torch.float32) , cap.to(\"cuda\")\n",
    "\n",
    "        with torch.no_grad():\n",
    "            latents = pipe.vae.encode(image).latent_dist.sample() * 0.18215\n",
    "            text = pipe.text_encoder(cap)[0]\n",
    "\n",
    "        noise = torch.randn_like(latents).to(\"cuda\", dtype=torch.float32)\n",
    "        timestep = torch.randint(0 , 1000 , (latents.shape[0],) , device = \"cuda\").long()\n",
    "        noise_latents = pipe.scheduler.add_noise(latents , noise , timestep)\n",
    "\n",
    "        pred = pipe.unet(noise_latents,\n",
    "                  timestep , \n",
    "                  text).sample\n",
    "        \n",
    "        loss = F.mse_loss(pred , noise , reduction = \"mean\")\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "        running_loss += loss.item()\n",
    "\n",
    "        print(f\"epoch{epoch} | loss: {loss.item()} | total loss {running_loss}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c36187d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cannot initialize model with low cpu memory usage because `accelerate` was not found in the environment. Defaulting to `low_cpu_mem_usage=False`. It is strongly recommended to install `accelerate` for faster and less memory-intense model loading. You can do so with: \n",
      "```\n",
      "pip install accelerate\n",
      "```\n",
      ".\n",
      "Loading pipeline components...:  29%|██▊       | 2/7 [00:02<00:05,  1.14s/it]`torch_dtype` is deprecated! Use `dtype` instead!\n",
      "Loading pipeline components...: 100%|██████████| 7/7 [00:04<00:00,  1.70it/s]\n",
      "C:\\Users\\sharm\\AppData\\Local\\Temp\\ipykernel_18516\\3714841406.py:5: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  lora_checkpoint = torch.load(r\"C:\\Users\\sharm\\Downloads\\dell\\finetune\\lora_only.pth\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully loaded Anime LoRA weights!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 84%|████████▍ | 31/37 [04:44<00:52,  8.73s/it]"
     ]
    }
   ],
   "source": [
    "# 1. Load the clean base model first\n",
    "pipe = StableDiffusionImg2ImgPipeline.from_pretrained(\"runwayml/stable-diffusion-v1-5\", torch_dtype=torch.float16).to(\"cuda\")\n",
    "\n",
    "# 2. Load your 90MB file\n",
    "lora_checkpoint = torch.load(r\"C:\\Users\\sharm\\Downloads\\dell\\finetune\\lora_only.pth\")\n",
    "\n",
    "# 3. Inject the weights (Strict=False is key here)\n",
    "# This ignores the base weights and only updates the LoRA layers\n",
    "pipe.unet.load_state_dict(lora_checkpoint, strict=False)\n",
    "print(\"Successfully loaded Anime LoRA weights!\")\n",
    "prompt = \"A professional anime portrait, sharp lines, studio ghibli style, high resolution\"\n",
    "# Use a lower guidance_scale (7-8) to see if the model follows your LoRA\n",
    "from PIL import Image\n",
    "\n",
    "# Load image properly\n",
    "image = Image.open(r\"C:\\Users\\sharm\\Downloads\\dell\\finetune\\as.png\").convert(\"RGB\")  # Must be RGB\n",
    "\n",
    "# Pass to pipeline\n",
    "result = pipe(\n",
    "    prompt=prompt,\n",
    "    image=image,  # PIL Image\n",
    "    strength=0.75\n",
    ").images[0]\n",
    "\n",
    "result.save(\"ak.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6591b50",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Method B: Save only LoRA weights (manual extraction)\n",
    "lora_weights = {}\n",
    "for name, param in pipe.unet.named_parameters():\n",
    "    if 'lora' in name.lower():\n",
    "        lora_weights[name] = param\n",
    "\n",
    "if lora_weights:\n",
    "    torch.save(lora_weights, \"lora_only.pth\")\n",
    "    print(f\"Saved {len(lora_weights)} LoRA parameters\")\n",
    "else:\n",
    "    print(\"No LoRA parameters found!\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gpt",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.25"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
